{"getPartners(94792d8b-146b-4c56-9fb0-05eeee97dfd5)":[],"getSubdemands(94792d8b-146b-4c56-9fb0-05eeee97dfd5)":[{"html":"<h4><span>The purpose and design of the algorithm</span></h4><p><span>A short (two or three sentence) description to explain what purpose(s) the company uses the algorithm for and why it has been preferred to other options.</span></p><p><span>An overview of the algorithm’s design should also be given including what sort of management decisions are made by the algorithm (and whether they are advisory or decisive); whether the algorithm relies on neural networks, machine learning, probabilistic functions or other type of logic; what training data was used; and under what circumstances the algorithm is not deployed or has a failsafe.</span></p>"},{"html":"<h4>The relative importance of the algorithm’s inputs and parameters</h4><p>The register must explain, in an accessible and non-technical way, what data and ratings it uses to reach decision. This means providing an easy way to understand how important different inputs and parameters are to different decisions. This could be done in various ways: from a simple rating of ‘high/medium/low importance’, to giving more specific and granular detail of the weighting each input or parameter has.&nbsp;</p><p>As well as explaining how important different parameters and inputs are, it should also explain the source of inputs (are they from the app, from customers, from the web, inferred, how long ago, while at work, from data brokers, etc) and in broad terms how they are calculated. Where inferred parameters (such as a risk score and other profiling parameters) are used to generate a particular algorithmic output, the company should provide a detailed description of the parameter and how it is generated, including the data used to determine its value. Fundamentally, complex inferred parameters should not be used as a mean to obscure the source and weight of data used in algorithms. Where decisions are based on the personal data or behaviour of workers and/or consumers this must be clear, and the source of any personal data used should also be set out. The register should also confirm that the algorithm uses only data that is strictly necessary for the purposes of the algorithm, and does not use any sensitive personal data, emotion recognition, data collected while not at work etc.</p><p>It is possible that AI algorithms will use parameters that are hard to give real-world human descriptions for. In such cases, the company must state this and instead thoroughly explain how the tool has been built, and how they monitor and audit its outputs to ensure that they do not result in bias or discrimination. Examples (or statistics) comparing different, but similar, inputs with differing outputs may also be needed to explain which sorts of inputs tend to lead to which sorts of outputs.<br>&nbsp;</p>"},{"html":"<h4>Human intervention</h4><p>Where algorithms are used to make decisions in the workplace, there should always be a human either checking, and/or able to review, any decisions.&nbsp;</p><p>The register should specify the responsibilities and roles of those human teams and must specify what level of decision-making authority oversight teams have, as well as the training that decision-makers have received particularly in respect of the design and potential impacts of the algorithm. The register should break down the level of decision-making authority across multiple stages of review and oversight (where &nbsp;applicable)The register should also provide some operational information about how much staff capacity (in FTE) is dedicated to human review and how long a review is expected to take.&nbsp;<br>&nbsp;</p>"},{"html":"<h4>Development history, updates, and impact assessments</h4><p>The company should also state where responsibilities for the development and updating of the algorithm lie, especially where an external supplier has been involved. This does not require identifying individuals, but rather relevant teams/departments/organisations and the nature of their different responsibilities. A log of updates should also be listed.<br>The register should also specify what if any consultation has taken place between the company and workers and their representatives with respect to the design or revision of the algorithm.</p><p>With regards to how this information is shared, we encourage the use of a variety of means such as flow charts, FAQs, or short form videos to accompany textual explanations. The selected mediums should strive to make understanding how the algorithm operates as accessible and simple as possible to those affected.<br>&nbsp;</p>"}],"getExamples(94792d8b-146b-4c56-9fb0-05eeee97dfd5)":[{"html":"<p>The below examples (and the subsequent ones accompanying each of the overarching demands) are without prejudice to our position that workers should not be forced to provide access to sensitive data, including for example biometric information, while at work, without due safeguards; and that they should not be subject to decision-making by opaque algorithms that impacts on their working conditions, in particular with respect to decisions concerning suspension and termination. This is why we have called for strong international regulation that safeguards against these practices, including most recently as part of the ILO's proposed new standard on decent platform work.</p>","imageURL":"https://privacyinternational.org/sites/default/files/flysystem/2024-11/11-PI_Website_Topic_Data%20Protection_3_0.jpg","imageAlt":"a repeating image of a phone with an arrow going out to symbolise data extraction"},{"html":"<h4>Case Scenario 1: Worker identification system</h4><h5>Purpose</h5><p>Ensure the person logged in and using the service to work is the person registered for this account. This system aims to confirm the account is used by the registered worker and not used by a different person. This was the preferred solution to keep costs manageable given the high number and high turnaround of workers using the platform. A third-party service was selected to avoid developing an in-house solution which would have to reach a high standard with regard to potential bias and false positives.</p><h5>Design</h5><p>Deterministic system relying on a third-party facial recognition service. This system captures photos of the users’ face and match it against previously stored photos of the account owner.</p><h5>Parameters and importance:</h5><ul><li>Biometrics data captured in the photo</li><li>metadata including device used to capture the photo, time, date</li><li>previously recorded account information such as ID, previously captured photos</li></ul><h5>Human Intervention:</h5><p>5 members of staff trained to review appeal by workers.</p><h5>Teams involved and means of contact:</h5><ul><li>In house Customer Identification team: notified of algorithmic decisions and challenges brough by workers. Can override a decision after investigation. Keeps a record and report bugs and other issues detected to the development team in charge of the system. Minimum training of 2 weeks required to be part of the team.</li><li>RH</li><li>Third party FRT development team: maintain and update the system, informed about bugs and issues</li><li>Data Protection team: Audits algorithm to ensure data collected and processed adheres to Privacy Policy and Data Protection Laws</li></ul><h5>First deployed</h5><p>01/03/2022</p><h5>Last major update</h5><p>24/07/2023</p><h5>Impact assessment</h5><p>Completed on 15/02/2022</p><h5>Engagement with workers and workers representative</h5><p>None</p>"},{"html":"<h4>Case Scenario 2: Account/contract termination</h4><h5>Purpose</h5><p>Terminate a workers’ contract following the satisfaction of a number of criteria. The aim of this system is to identify accounts that do not match the standards set by the company and should have their account terminated</p><h5>Design:</h5><p>Deterministic system that monitors reviews, reports and other parameters to automatically flag workers who reach a defined threshold</p><h5>Parameters and importance:</h5><ul><li>Number and quality of reports from clients having interacted with the worker - High</li><li>Feedback from clients having interacted with the worker - High</li><li>Number of hours active on the platform - medium</li><li>Number of jobs performed by the worker - medium</li><li>Geolocation data - low</li><li>…</li></ul><h5>Teams involved and means of contact:</h5><ul><li>Engineering team</li><li>RH team</li><li>Data Protection team</li><li>…</li></ul><h5>Human Intervention:</h5><p>High, a human will always review the decision taken by the algorithm and make the final decision based on both the information provided by the system and its own interpretation of the data that triggered the decision to flag a worker.</p><h5>Staff allocated:</h5><p>10 people with specific training</p><h5>Decision overrun by human review:</h5><p>10% from 2021 to 2024</p><h5>First deployed:</h5><p>25/10/2023</p><h5>Last major update:</h5><p>14/11/2023</p><h5>Impact assessment:</h5><p>Completed on 01/02/2022</p><h5>Engagement with workers and workers representative:</h5><p>Yes. Met with Union 1 between November and December 2021. Circulated survey to workers during November and December 2021.</p>"},{}],"getMicrositeNodesMap()":{"processedNodes":[{"id":"94792d8b-146b-4c56-9fb0-05eeee97dfd5","pageType":"Demand","shortTitle":"Public register","body":{"value":"<p><span>The public register is key to addressing the information imbalance of algorithmic management by allowing workers (and candidates) and their representatives to understand what algorithms are being used and how they work. In order to do this, the register must be in accessible non-technical language and kept up to date. It must include a list of all algorithms that affect worker's treatment</span><a><span>&nbsp;</span></a><span>while at work. For each listed algorithm, the following information must be included:</span></p><p><span>This is different from the previous sentence. I think we should agree on a term and stick to it to avoid any confusion. We could even define it. e.g: “algorithms that affect workers. We define those as any algorithm that produces, or is part of a larger system that produces, outputs influencing&nbsp; the conditions under which workers access work, are managed, perform their work, affect their working conditions or that influence their working status”. This could be included in the top para rather than here as it applies for all demands</span></p>","format":"basic_html","processed":"<p><span>The public register is key to addressing the information imbalance of algorithmic management by allowing workers (and candidates) and their representatives to understand what algorithms are being used and how they work. In order to do this, the register must be in accessible non-technical language and kept up to date. It must include a list of all algorithms that affect worker's treatment</span><a><span>&nbsp;</span></a><span>while at work. For each listed algorithm, the following information must be included:</span></p><p><span>This is different from the previous sentence. I think we should agree on a term and stick to it to avoid any confusion. We could even define it. e.g: “algorithms that affect workers. We define those as any algorithm that produces, or is part of a larger system that produces, outputs influencing&nbsp; the conditions under which workers access work, are managed, perform their work, affect their working conditions or that influence their working status”. This could be included in the top para rather than here as it applies for all demands</span></p>","summary":""},"title":"Maintain a public register of the algorithms used to manage workers","metatag":[{"tag":"meta","attributes":{"name":"title","content":"Maintain a public register of the algorithms used to manage workers | Privacy International"}},{"tag":"meta","attributes":{"name":"description","content":"The public register is key to addressing the information imbalance of algorithmic management by allowing workers (and candidates) and their representatives to understand what algorithms are being used and how they work. In order to do this, the register must be in accessible non-technical language and kept up to date. It must include a list of all algorithms that affect worker's treatment while at work. For each listed algorithm, the following information must be included:"}},{"tag":"link","attributes":{"rel":"canonical","href":"http://privacyinternational.org/node/5358"}},{"tag":"link","attributes":{"rel":"image_src","href":"http://privacyinternational.org/sites/default/files/styles/large/public/2024-08/igor-omilaev-_VOotpSw5nc-unsplash.jpg?itok=rGfzKeXo"}},{"tag":"meta","attributes":{"name":"rights","content":"Creative Commons Attribution-ShareAlike 4.0 International"}},{"tag":"link","attributes":{"rel":"icon","href":"/sites/default/files/fav/favicon.ico"}},{"tag":"link","attributes":{"rel":"icon","sizes":"16x16","href":"/sites/default/files/fav/favicon-16x16.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"32x32","href":"/sites/default/files/fav/favicon-32x32.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"96x96","href":"/sites/default/files/fav/favicon-96x96.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"192x192","href":"/sites/default/files/fav/android-icon-192x192.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","href":"/sites/default/files/fav/apple-icon-60x60.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-144x144.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","href":"/sites/default/files/fav/apple-icon-57x57.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"meta","attributes":{"property":"og:site_name","content":"Privacy International"}},{"tag":"meta","attributes":{"property":"og:url","content":"http://privacyinternational.org/node/5358"}},{"tag":"meta","attributes":{"property":"og:title","content":"Maintain a public register of the algorithms used to manage workers"}},{"tag":"meta","attributes":{"property":"og:description","content":"The public register is key to addressing the information imbalance of algorithmic management by allowing workers (and candidates) and their representatives to understand what algorithms are being used and how they work. In order to do this, the register must be in accessible non-technical language and kept up to date. It must include a list of all algorithms that affect worker's treatment while at work. For each listed algorithm, the following information must be included:"}},{"tag":"meta","attributes":{"property":"og:image:url","content":"http://privacyinternational.org/sites/default/files/styles/large/public/2024-08/igor-omilaev-_VOotpSw5nc-unsplash.jpg?itok=rGfzKeXo"}},{"tag":"meta","attributes":{"property":"og:image:secure_url","content":"https://privacyinternational.org/sites/default/files/styles/large/public/2024-08/igor-omilaev-_VOotpSw5nc-unsplash.jpg?itok=rGfzKeXo"}},{"tag":"meta","attributes":{"property":"og:image:width","content":"2100"}},{"tag":"meta","attributes":{"property":"og:image:height","content":"2688"}},{"tag":"meta","attributes":{"property":"og:image:alt","content":"Uniform grid of endless rows of desk cubicles with laptops and monitors"}},{"tag":"meta","attributes":{"name":"twitter:card","content":"summary_large_image"}},{"tag":"meta","attributes":{"name":"twitter:description","content":"The public register is key to addressing the information imbalance of algorithmic management by allowing workers (and candidates) and their representatives to understand what algorithms are being used and how they work. In order to do this, the register must be in accessible non-technical language and kept up to date. It must include a list of all algorithms that affect worker's treatment while at work. For each listed algorithm, the following information must be included:"}},{"tag":"meta","attributes":{"name":"twitter:title","content":"Maintain a public register of the algorithms used to manage workers"}},{"tag":"meta","attributes":{"name":"twitter:site","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:site:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:dnt","content":"on"}},{"tag":"meta","attributes":{"name":"twitter:image:alt","content":"Uniform grid of endless rows of desk cubicles with laptops and monitors"}},{"tag":"meta","attributes":{"name":"twitter:image","content":"http://privacyinternational.org/sites/default/files/styles/large/public/2024-08/igor-omilaev-_VOotpSw5nc-unsplash.jpg?itok=rGfzKeXo"}}],"examplesTitle":"Examples","imageURL":"https://privacyinternational.org/sites/default/files/2024-08/igor-omilaev-_VOotpSw5nc-unsplash.jpg","imageAlt":"Uniform grid of endless rows of desk cubicles with laptops and monitors"},{"id":"168337d4-184c-45d9-a681-2e05edf0af67","pageType":"Demand","shortTitle":"Explanations","body":{"value":"<p>A reason for the decision should always be made available to the worker, including by providing data about the inputs, including worker personal data, and parameters that were decisive to the outcome or that could have resulted in a different outcome. Sources of particular parameters and inputs must also be provided and explained – for example in the event that a decision is based on a customer feedback rating. Reasons given for a particular decision must be specific rather than wholly generic and should not be provided in overly technical language.&nbsp;</p><p>Where an algorithmically generated score was used in relation to a decision – companies should provide workers with the overall distribution ratio of the score – i.e. how many workers fall into the low/medium/high risk categories within a given geographic area (for example the city in which the worker is operating). The purpose of this is to provide the context behind decision-making, which would in turn uphold algorithmic accountability and enable workers to challenge inaccurate parameters and inputs. This information could be provided through an aggregated percentage of workers with a certain score or rating. Given that this information is likely to change over time, the ratio should be provided to workers at the time that they face a particular decision, such as termination. Similarly, the company should also provide information that addresses the prevalence and complexity of any parameters that prompted a particular automated output at the time of the decision. For example, if a company flags a worker on suspicion of GPS spoofing and suspends his account as a result – it should provide information on whether other workers within the same geographical location also reported technical issues relating to the app’s collection of location data.</p><p>The purpose of this is not just to address information asymmetry and allow decisions to be challenged, but also to allow workers to understand why they are being treated a certain way and what changes they can make to get a better outcome. This doesn’t necessarily mean going into the details of the algorithm, but rather providing insight into what change(s) a worker could make to receive a more desirable outcome in the future.</p><p>A worker should be able to challenge any decision they think is wrong or unfair. Contact details of a human must be provided of who to contact for this. As such, workers must be provided with information as regards which teams have what oversight over the algorithm’s outputs and how they can be contacted. This information must also include the job titles and relative seniority of particular human agents involved in the review and oversight of the decision relating to the worker.&nbsp;<br>&nbsp;</p>","format":"basic_html","processed":"<p>A reason for the decision should always be made available to the worker, including by providing data about the inputs, including worker personal data, and parameters that were decisive to the outcome or that could have resulted in a different outcome. Sources of particular parameters and inputs must also be provided and explained – for example in the event that a decision is based on a customer feedback rating. Reasons given for a particular decision must be specific rather than wholly generic and should not be provided in overly technical language.&nbsp;</p><p>Where an algorithmically generated score was used in relation to a decision – companies should provide workers with the overall distribution ratio of the score – i.e. how many workers fall into the low/medium/high risk categories within a given geographic area (for example the city in which the worker is operating). The purpose of this is to provide the context behind decision-making, which would in turn uphold algorithmic accountability and enable workers to challenge inaccurate parameters and inputs. This information could be provided through an aggregated percentage of workers with a certain score or rating. Given that this information is likely to change over time, the ratio should be provided to workers at the time that they face a particular decision, such as termination. Similarly, the company should also provide information that addresses the prevalence and complexity of any parameters that prompted a particular automated output at the time of the decision. For example, if a company flags a worker on suspicion of GPS spoofing and suspends his account as a result – it should provide information on whether other workers within the same geographical location also reported technical issues relating to the app’s collection of location data.</p><p>The purpose of this is not just to address information asymmetry and allow decisions to be challenged, but also to allow workers to understand why they are being treated a certain way and what changes they can make to get a better outcome. This doesn’t necessarily mean going into the details of the algorithm, but rather providing insight into what change(s) a worker could make to receive a more desirable outcome in the future.</p><p>A worker should be able to challenge any decision they think is wrong or unfair. Contact details of a human must be provided of who to contact for this. As such, workers must be provided with information as regards which teams have what oversight over the algorithm’s outputs and how they can be contacted. This information must also include the job titles and relative seniority of particular human agents involved in the review and oversight of the decision relating to the worker.&nbsp;<br>&nbsp;</p>","summary":""},"title":"Accompany all algorithmic decisions with an explanation of the most important reasons and/or parameter(s) behind the decision and how they can be challenged","metatag":[{"tag":"meta","attributes":{"name":"title","content":"Accompany all algorithmic decisions with an explanation of the most important reasons and/or parameter(s) behind the decision and how they can be challenged | Privacy International"}},{"tag":"meta","attributes":{"name":"description","content":"A reason for the decision should always be made available to the worker, including by providing data about the inputs, including worker personal data, and parameters that were decisive to the outcome or that could have resulted in a different outcome. Sources of particular parameters and inputs must also be provided and explained – for example in the event that a decision is based on a customer feedback rating. Reasons given for a particular decision must be specific rather than wholly generic and should not be provided in overly technical language. "}},{"tag":"link","attributes":{"rel":"canonical","href":"http://privacyinternational.org/node/5458"}},{"tag":"link","attributes":{"rel":"image_src","href":"http://privacyinternational.org/sites/default/files/styles/large/public/2024-10/claudio-schwarz-fyeOxvYvIyY-unsplash.jpg?itok=FVhL61GA"}},{"tag":"meta","attributes":{"name":"rights","content":"Creative Commons Attribution-ShareAlike 4.0 International"}},{"tag":"link","attributes":{"rel":"icon","href":"/sites/default/files/fav/favicon.ico"}},{"tag":"link","attributes":{"rel":"icon","sizes":"16x16","href":"/sites/default/files/fav/favicon-16x16.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"32x32","href":"/sites/default/files/fav/favicon-32x32.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"96x96","href":"/sites/default/files/fav/favicon-96x96.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"192x192","href":"/sites/default/files/fav/android-icon-192x192.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","href":"/sites/default/files/fav/apple-icon-60x60.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-144x144.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","href":"/sites/default/files/fav/apple-icon-57x57.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"meta","attributes":{"property":"og:site_name","content":"Privacy International"}},{"tag":"meta","attributes":{"property":"og:url","content":"http://privacyinternational.org/node/5458"}},{"tag":"meta","attributes":{"property":"og:title","content":"Accompany all algorithmic decisions with an explanation of the most important reasons and/or parameter(s) behind the decision and how they can be challenged"}},{"tag":"meta","attributes":{"property":"og:description","content":"A reason for the decision should always be made available to the worker, including by providing data about the inputs, including worker personal data, and parameters that were decisive to the outcome or that could have resulted in a different outcome. Sources of particular parameters and inputs must also be provided and explained – for example in the event that a decision is based on a customer feedback rating. Reasons given for a particular decision must be specific rather than wholly generic and should not be provided in overly technical language. "}},{"tag":"meta","attributes":{"property":"og:image:url","content":"http://privacyinternational.org/sites/default/files/styles/large/public/2024-10/claudio-schwarz-fyeOxvYvIyY-unsplash.jpg?itok=FVhL61GA"}},{"tag":"meta","attributes":{"property":"og:image:secure_url","content":"https://privacyinternational.org/sites/default/files/styles/large/public/2024-10/claudio-schwarz-fyeOxvYvIyY-unsplash.jpg?itok=FVhL61GA"}},{"tag":"meta","attributes":{"property":"og:image:width","content":"1280"}},{"tag":"meta","attributes":{"property":"og:image:height","content":"1920"}},{"tag":"meta","attributes":{"property":"og:image:alt","content":"Image saying data"}},{"tag":"meta","attributes":{"name":"twitter:card","content":"summary_large_image"}},{"tag":"meta","attributes":{"name":"twitter:description","content":"A reason for the decision should always be made available to the worker, including by providing data about the inputs, including worker personal data, and parameters that were decisive to the outcome or that could have resulted in a different outcome. Sources of particular parameters and inputs must also be provided and explained – for example in the event that a decision is based on a customer feedback rating. Reasons given for a particular decision must be specific rather than wholly generic and should not be provided in overly technical language. "}},{"tag":"meta","attributes":{"name":"twitter:title","content":"Accompany all algorithmic decisions with an explanation of the most important reasons and/or parameter(s) behind the decision and how they can be challenged"}},{"tag":"meta","attributes":{"name":"twitter:site","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:site:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:dnt","content":"on"}},{"tag":"meta","attributes":{"name":"twitter:image:alt","content":"Image saying data"}},{"tag":"meta","attributes":{"name":"twitter:image","content":"http://privacyinternational.org/sites/default/files/styles/large/public/2024-10/claudio-schwarz-fyeOxvYvIyY-unsplash.jpg?itok=FVhL61GA"}}],"examplesTitle":"Case Studies","imageURL":"https://privacyinternational.org/sites/default/files/2024-10/claudio-schwarz-fyeOxvYvIyY-unsplash.jpg","imageAlt":"Image saying data"},{"id":"3c40115a-c4a8-4a2d-9696-9be27e0dd87d","pageType":"Demand","shortTitle":"Testing","body":{"value":"<p><span>This could be done by providing API access to a sandboxed version of the system, making open source the key algorithms used on the platform or provide access to anonymised/synthetic data accurately reflecting the behaviour of the system. Companies should also consider sharing their source code and training datasets directly to further improve transparency and accountability.</span></p><p><span>While public access would be a gold standard, a more limited approach may be appropriate, in which only worker representative bodies or recognised academic or civil society organisations can access the testbeds, potentially via licensed access. The ability for workers to test how the algorithms work cannot be confined to a single instance and instead there should be a repeated possibility to trial relevant systems via a sandboxed version. Where applicable, the arrangements governing access to and trialling of algorithms by those affected by them could be set out by way of collective agreements between worker representative bodies and companies.&nbsp;</span></p><p><span>Adequate documentation should be provided to make use of these resources.</span></p>","format":"basic_html","processed":"<p><span>This could be done by providing API access to a sandboxed version of the system, making open source the key algorithms used on the platform or provide access to anonymised/synthetic data accurately reflecting the behaviour of the system. Companies should also consider sharing their source code and training datasets directly to further improve transparency and accountability.</span></p><p><span>While public access would be a gold standard, a more limited approach may be appropriate, in which only worker representative bodies or recognised academic or civil society organisations can access the testbeds, potentially via licensed access. The ability for workers to test how the algorithms work cannot be confined to a single instance and instead there should be a repeated possibility to trial relevant systems via a sandboxed version. Where applicable, the arrangements governing access to and trialling of algorithms by those affected by them could be set out by way of collective agreements between worker representative bodies and companies.&nbsp;</span></p><p><span>Adequate documentation should be provided to make use of these resources.</span></p>","summary":""},"title":"Allow workers, their representatives and public interest groups to test how the algorithms work","metatag":[{"tag":"meta","attributes":{"name":"title","content":"Allow workers, their representatives and public interest groups to test how the algorithms work | Privacy International"}},{"tag":"meta","attributes":{"name":"description","content":"This could be done by providing API access to a sandboxed version of the system, making open source the key algorithms used on the platform or provide access to anonymised/synthetic data accurately reflecting the behaviour of the system. Companies should also consider sharing their source code and training datasets directly to further improve transparency and accountability."}},{"tag":"link","attributes":{"rel":"canonical","href":"http://privacyinternational.org/node/5459"}},{"tag":"link","attributes":{"rel":"image_src","href":"http://privacyinternational.org/sites/default/files/styles/large/public/2024-11/mw-QxGSSfatjRs-unsplash.jpg?itok=N1bopjqA"}},{"tag":"meta","attributes":{"name":"rights","content":"Creative Commons Attribution-ShareAlike 4.0 International"}},{"tag":"link","attributes":{"rel":"icon","href":"/sites/default/files/fav/favicon.ico"}},{"tag":"link","attributes":{"rel":"icon","sizes":"16x16","href":"/sites/default/files/fav/favicon-16x16.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"32x32","href":"/sites/default/files/fav/favicon-32x32.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"96x96","href":"/sites/default/files/fav/favicon-96x96.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"192x192","href":"/sites/default/files/fav/android-icon-192x192.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","href":"/sites/default/files/fav/apple-icon-60x60.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-144x144.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","href":"/sites/default/files/fav/apple-icon-57x57.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"meta","attributes":{"property":"og:site_name","content":"Privacy International"}},{"tag":"meta","attributes":{"property":"og:url","content":"http://privacyinternational.org/node/5459"}},{"tag":"meta","attributes":{"property":"og:title","content":"Allow workers, their representatives and public interest groups to test how the algorithms work"}},{"tag":"meta","attributes":{"property":"og:description","content":"This could be done by providing API access to a sandboxed version of the system, making open source the key algorithms used on the platform or provide access to anonymised/synthetic data accurately reflecting the behaviour of the system. Companies should also consider sharing their source code and training datasets directly to further improve transparency and accountability."}},{"tag":"meta","attributes":{"property":"og:image:url","content":"http://privacyinternational.org/sites/default/files/styles/large/public/2024-11/mw-QxGSSfatjRs-unsplash.jpg?itok=N1bopjqA"}},{"tag":"meta","attributes":{"property":"og:image:secure_url","content":"https://privacyinternational.org/sites/default/files/styles/large/public/2024-11/mw-QxGSSfatjRs-unsplash.jpg?itok=N1bopjqA"}},{"tag":"meta","attributes":{"property":"og:image:width","content":"1920"}},{"tag":"meta","attributes":{"property":"og:image:height","content":"1920"}},{"tag":"meta","attributes":{"property":"og:image:alt","content":"A grid of heart monitors"}},{"tag":"meta","attributes":{"name":"twitter:card","content":"summary_large_image"}},{"tag":"meta","attributes":{"name":"twitter:description","content":"This could be done by providing API access to a sandboxed version of the system, making open source the key algorithms used on the platform or provide access to anonymised/synthetic data accurately reflecting the behaviour of the system. Companies should also consider sharing their source code and training datasets directly to further improve transparency and accountability."}},{"tag":"meta","attributes":{"name":"twitter:title","content":"Allow workers, their representatives and public interest groups to test how the algorithms work"}},{"tag":"meta","attributes":{"name":"twitter:site","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:site:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:dnt","content":"on"}},{"tag":"meta","attributes":{"name":"twitter:image:alt","content":"A grid of heart monitors"}},{"tag":"meta","attributes":{"name":"twitter:image","content":"http://privacyinternational.org/sites/default/files/styles/large/public/2024-11/mw-QxGSSfatjRs-unsplash.jpg?itok=N1bopjqA"}}],"examplesTitle":"Examples","imageURL":"https://privacyinternational.org/sites/default/files/2024-11/mw-QxGSSfatjRs-unsplash.jpg","imageAlt":"A grid of heart monitors"},{"id":"c6268060-b0df-4323-8990-9db860e2daa0","pageType":"Homepage","shortTitle":null,"body":{"value":"<p>Dear [insert names or companies here](,</p><p>We, the undersigned, believe that companies should respect their workers. We believe that <strong>you</strong> should respect <strong>your</strong> workers.</p><p>Each one of you are a market leader. And each of you claim to care, variously promising you ‘<a href=\"https://www.justeattakeaway.com/responsible-business\">believe in doing business responsibly and having a positive impact</a>’, that you will ‘<a href=\"https://corporate.deliveroo.co.uk/\">put the voice of the rider at the heart of everything</a>’ or ‘<a href=\"https://investor.uber.com/a-letter-from-our-ceo/?uclick_id=fe316402-6678-4c84-929f-53145a00b124\">will ensure that we treat our customers, our colleagues, and our cities with respect; and […] will run our business with passion, humility, and integrity</a>’.</p><p><a class=\"button\" href=\"https://privacyinternational.org\" target=\"_blank\">Read our Open letter</a></p><p>But this has yet to be borne out in your business practices, which have lead to millions of euros in fines for <a href=\"https://finance.yahoo.com/news/dutch-watchdog-fines-uber-10-121641049.html?guccounter=1\">obstructing drivers’ attempts to enforce their rights</a>, and for <a href=\"https://iapp.org/news/a/what-can-we-learn-from-the-garantes-recent-2-5m-euro-fine\">systemically inappropriate data processing</a>, along with <a href=\"https://www.theguardian.com/business/2023/apr/22/fired-by-ai-just-eat-uk-couriers-deactivated-for-minor-overpayments\">de-activating accounts by automated systems for minor overpayments</a>.</p><p>Instead, you are automating exploitation - leveraging black box algorithms to make decisions about de-activation, work allocation and pay without sufficient explanation, stripping workers of the ability to understand and challenge those decisions.</p><p>We believe the foundation of repect is transparency. Yet current systems withhold vital information from workers - creating precarity, stress, and misery.</p><p>We believe a responsible employer should:</p>","format":"full_html","processed":"<p>Dear [insert names or companies here](,</p><p>We, the undersigned, believe that companies should respect their workers. We believe that <strong>you</strong> should respect <strong>your</strong> workers.</p><p>Each one of you are a market leader. And each of you claim to care, variously promising you ‘<a href=\"https://www.justeattakeaway.com/responsible-business\">believe in doing business responsibly and having a positive impact</a>’, that you will ‘<a href=\"https://corporate.deliveroo.co.uk/\">put the voice of the rider at the heart of everything</a>’ or ‘<a href=\"https://investor.uber.com/a-letter-from-our-ceo/?uclick_id=fe316402-6678-4c84-929f-53145a00b124\">will ensure that we treat our customers, our colleagues, and our cities with respect; and […] will run our business with passion, humility, and integrity</a>’.</p><p><a class=\"button\" href=\"https://privacyinternational.org\" target=\"_blank\">Read our Open letter</a></p><p>But this has yet to be borne out in your business practices, which have lead to millions of euros in fines for <a href=\"https://finance.yahoo.com/news/dutch-watchdog-fines-uber-10-121641049.html?guccounter=1\">obstructing drivers’ attempts to enforce their rights</a>, and for <a href=\"https://iapp.org/news/a/what-can-we-learn-from-the-garantes-recent-2-5m-euro-fine\">systemically inappropriate data processing</a>, along with <a href=\"https://www.theguardian.com/business/2023/apr/22/fired-by-ai-just-eat-uk-couriers-deactivated-for-minor-overpayments\">de-activating accounts by automated systems for minor overpayments</a>.</p><p>Instead, you are automating exploitation - leveraging black box algorithms to make decisions about de-activation, work allocation and pay without sufficient explanation, stripping workers of the ability to understand and challenge those decisions.</p><p>We believe the foundation of repect is transparency. Yet current systems withhold vital information from workers - creating precarity, stress, and misery.</p><p>We believe a responsible employer should:</p>","summary":""},"title":"Transparency and explainability for algorithmic decisions at work","metatag":[{"tag":"meta","attributes":{"name":"title","content":"Transparency and explainability for algorithmic decisions at work | Privacy International"}},{"tag":"meta","attributes":{"name":"description","content":"Dear [insert names or companies here](,We, the undersigned, believe that companies should respect their workers. We believe that you should respect your workers."}},{"tag":"link","attributes":{"rel":"canonical","href":"http://privacyinternational.org/node/5465"}},{"tag":"meta","attributes":{"name":"rights","content":"Creative Commons Attribution-ShareAlike 4.0 International"}},{"tag":"link","attributes":{"rel":"icon","href":"/sites/default/files/fav/favicon.ico"}},{"tag":"link","attributes":{"rel":"icon","sizes":"16x16","href":"/sites/default/files/fav/favicon-16x16.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"32x32","href":"/sites/default/files/fav/favicon-32x32.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"96x96","href":"/sites/default/files/fav/favicon-96x96.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"192x192","href":"/sites/default/files/fav/android-icon-192x192.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","href":"/sites/default/files/fav/apple-icon-60x60.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-144x144.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","href":"/sites/default/files/fav/apple-icon-57x57.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"meta","attributes":{"property":"og:site_name","content":"Privacy International"}},{"tag":"meta","attributes":{"property":"og:url","content":"http://privacyinternational.org/node/5465"}},{"tag":"meta","attributes":{"property":"og:title","content":"Transparency and explainability for algorithmic decisions at work"}},{"tag":"meta","attributes":{"property":"og:description","content":"Dear [insert names or companies here](,We, the undersigned, believe that companies should respect their workers. We believe that you should respect your workers."}},{"tag":"meta","attributes":{"name":"twitter:card","content":"summary_large_image"}},{"tag":"meta","attributes":{"name":"twitter:description","content":"Dear [insert names or companies here](,We, the undersigned, believe that companies should respect their workers. We believe that you should respect your workers."}},{"tag":"meta","attributes":{"name":"twitter:title","content":"Transparency and explainability for algorithmic decisions at work"}},{"tag":"meta","attributes":{"name":"twitter:site","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:site:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:dnt","content":"on"}}],"examplesTitle":null},{"id":"c2fd97d8-0386-47a3-9fd3-2b8ff7a31e82","pageType":"About","shortTitle":null,"body":{"value":"<p>Millions of people worldwide&nbsp;are working in the gig economy sector for companies like Uber, Deliveroo, Bolt, Just Eat… And this could be the future of work for people working outside the gig economy, as surveillance technologies are creeping into the workplace – and the ‘work-from-home place’ in particular.</p><p>To counter the surveillance that employers are subjecting workers to, and the power imbalance that workers face, Privacy International (PI) has partnered with&nbsp;Worker Info Exchange&nbsp;and&nbsp;App Drivers and Couriers Union, who have been working on these issues and fighting to protect rights of gig economy workers.</p><p>To understand the issues faced by gig economy workers it is important to first understand how companies collect and use data to make decisions about their workers, including how work is allocated, how much money drivers are able to earn, and more.</p>","format":"basic_html","processed":"<p>Millions of people worldwide&nbsp;are working in the gig economy sector for companies like Uber, Deliveroo, Bolt, Just Eat… And this could be the future of work for people working outside the gig economy, as surveillance technologies are creeping into the workplace – and the ‘work-from-home place’ in particular.</p><p>To counter the surveillance that employers are subjecting workers to, and the power imbalance that workers face, Privacy International (PI) has partnered with&nbsp;Worker Info Exchange&nbsp;and&nbsp;App Drivers and Couriers Union, who have been working on these issues and fighting to protect rights of gig economy workers.</p><p>To understand the issues faced by gig economy workers it is important to first understand how companies collect and use data to make decisions about their workers, including how work is allocated, how much money drivers are able to earn, and more.</p>","summary":""},"title":"About us","metatag":[{"tag":"meta","attributes":{"name":"title","content":"About us | Privacy International"}},{"tag":"meta","attributes":{"name":"description","content":"Millions of people worldwide are working in the gig economy sector for companies like Uber, Deliveroo, Bolt, Just Eat… And this could be the future of work for people working outside the gig economy, as surveillance technologies are creeping into the workplace – and the ‘work-from-home place’ in particular."}},{"tag":"link","attributes":{"rel":"canonical","href":"http://privacyinternational.org/node/5466"}},{"tag":"meta","attributes":{"name":"rights","content":"Creative Commons Attribution-ShareAlike 4.0 International"}},{"tag":"link","attributes":{"rel":"icon","href":"/sites/default/files/fav/favicon.ico"}},{"tag":"link","attributes":{"rel":"icon","sizes":"16x16","href":"/sites/default/files/fav/favicon-16x16.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"32x32","href":"/sites/default/files/fav/favicon-32x32.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"96x96","href":"/sites/default/files/fav/favicon-96x96.png"}},{"tag":"link","attributes":{"rel":"icon","sizes":"192x192","href":"/sites/default/files/fav/android-icon-192x192.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","href":"/sites/default/files/fav/apple-icon-60x60.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-144x144.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","href":"/sites/default/files/fav/apple-icon-57x57.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"72x72","href":"/sites/default/files/fav/apple-icon-72x72.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"76x76","href":"/sites/default/files/fav/apple-icon-76x76.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"114x114","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"120x120","href":"/sites/default/files/fav/apple-icon-120x120.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"144x144","href":"/sites/default/files/fav/apple-icon-114x114.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"152x152","href":"/sites/default/files/fav/apple-icon-152x152.png"}},{"tag":"link","attributes":{"rel":"apple-touch-icon-precomposed","sizes":"180x180","href":"/sites/default/files/fav/apple-icon-180x180.png"}},{"tag":"meta","attributes":{"property":"og:site_name","content":"Privacy International"}},{"tag":"meta","attributes":{"property":"og:url","content":"http://privacyinternational.org/node/5466"}},{"tag":"meta","attributes":{"property":"og:title","content":"About us"}},{"tag":"meta","attributes":{"property":"og:description","content":"Millions of people worldwide are working in the gig economy sector for companies like Uber, Deliveroo, Bolt, Just Eat… And this could be the future of work for people working outside the gig economy, as surveillance technologies are creeping into the workplace – and the ‘work-from-home place’ in particular."}},{"tag":"meta","attributes":{"name":"twitter:card","content":"summary_large_image"}},{"tag":"meta","attributes":{"name":"twitter:description","content":"Millions of people worldwide are working in the gig economy sector for companies like Uber, Deliveroo, Bolt, Just Eat… And this could be the future of work for people working outside the gig economy, as surveillance technologies are creeping into the workplace – and the ‘work-from-home place’ in particular."}},{"tag":"meta","attributes":{"name":"twitter:title","content":"About us"}},{"tag":"meta","attributes":{"name":"twitter:site","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:site:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator:id","content":"20982910"}},{"tag":"meta","attributes":{"name":"twitter:creator","content":"@privacyint"}},{"tag":"meta","attributes":{"name":"twitter:dnt","content":"on"}}],"examplesTitle":null}],"indexesMap":{"c6268060-b0df-4323-8990-9db860e2daa0":3,"c2fd97d8-0386-47a3-9fd3-2b8ff7a31e82":4,"94792d8b-146b-4c56-9fb0-05eeee97dfd5":0,"3c40115a-c4a8-4a2d-9696-9be27e0dd87d":2,"168337d4-184c-45d9-a681-2e05edf0af67":1},"homepageIndex":3,"aboutPageIndex":4},"getSocials()":[],"getSubdemands(3c40115a-c4a8-4a2d-9696-9be27e0dd87d)":[],"getExamples(3c40115a-c4a8-4a2d-9696-9be27e0dd87d)":[{"html":"<h4><span>Case Scenario 1</span></h4><p><span>A company offers a taxi drivers and passenger matching service. To allow drivers and their representative to understand how the matching algorithm functions, they provide a sandboxed version of the algorithm over an API. The sandboxed version allows authenticated users to simulate client requests and the location of drivers, and evaluate how these demands propagate to drivers, including how this information is presented to each driver individually.</span></p><p><span>The API allow users to import data for easy testing of complex scenarios and allows export of data to analyse how the algorithm reacts to different conditions. The company provide a feedback mechanism for API users to report bugs or issues.</span></p><p><span>To enable better auditing of the algorithm, the company provides access to the three latest version of the system so that users can compare results.</span></p><p><span>Individuals and organisations must go through a selection process offered by the company to be authorised access the API and are submitted to Terms and Conditions as to limit potential damage.</span></p><p>&nbsp;</p>"}],"getPartners(3c40115a-c4a8-4a2d-9696-9be27e0dd87d)":[],"getSubdemands(c2fd97d8-0386-47a3-9fd3-2b8ff7a31e82)":[],"getExamples(c2fd97d8-0386-47a3-9fd3-2b8ff7a31e82)":[{}],"getPartners(c2fd97d8-0386-47a3-9fd3-2b8ff7a31e82)":[{"html":"<p>Digital rights are increasingly important in the fight for worker rights for gig economy workers who are managed and controlled at work by hidden algorithms. Opaque automated management systems foster exploitation and discrimination. We challenge these practices through three strands of activity</p>","title":"Worker Info Exchange","url":"https://www.workerinfoexchange.org","imageURL":"https://privacyinternational.org/sites/default/files/flysystem/2024-11/5b88ae_4423f5198d9543edb04d25c346281fb7~mv2_0.png","imageAlt":"Worker info Exchange logo"},{"html":"<p>Governments and corporations are using technology to exploit us. Their abuses of power threaten our freedoms and the very things that make us human.&nbsp;</p><p>That’s why PI is here:&nbsp;to protect democracy, defend people's dignity, and demand accountability from institutions who breach public trust.&nbsp;</p>","title":"Privacy International","url":"https://privacyinternational.org/","imageURL":"https://privacyinternational.org/sites/default/files/flysystem/2024-11/MAIN%20LOGO%20FILE%20-%20PI%20Logo%20Roundel%20Flat%20RGB%20Solid_0.png","imageAlt":"PI Logo"}],"getPartners(168337d4-184c-45d9-a681-2e05edf0af67)":[],"getSubdemands(168337d4-184c-45d9-a681-2e05edf0af67)":[],"getExamples(168337d4-184c-45d9-a681-2e05edf0af67)":[{"html":"<h4><span>&nbsp;Case Scenario 1</span></h4><p><span>A driver is refused access to their account after taking a photo of themselves following a prompt by the platform. The driver should be provided with the key parameters that led to this decision if appropriate, for example: what existing data the photo was compared against, match percentage, metadata (such as device maker and model).</span></p><p><span>In this case scenario, they also should be informed that the elements captured and shared with the third-party service in charge of authentication were not sufficient to confirm their identity.</span></p><p><span>The worker should be given the possibility to immediately contest this decision and have it reviewed by a human or be provided with an alternative way to verify their identity. Should the decision be overturned, appropriate compensation should be offered to make up for the time lost by the worker.</span></p>"},{"html":"<h4><span>Case Scenario 2</span></h4><p><span>A courier is notified that their account has been de-activated. The notification should provide all the relevant information that led to this decision, for example that their conduct has been flagged and reported by multiple customers following an identified pattern and that a human staff member acted on the report. The courier should be provided with the relevant data, including number of reports, time, date and general reason for this report.&nbsp;</span></p><p><span>The notification must also identify any review and oversight team(s) involved in the de-activation determination. This information should also include the relative seniority and job titles of any human agents involved in the decision as well as the length of time taken to review the de-activation decision by each respective team(s) and an explanation of any escalation process between teams (if applicable).&nbsp;</span></p><p><span>Within the notification, or easily accessible, should be a mean to contest this decision with adequate inputs allowing the worker to provide additional information.</span></p>"},{"html":"<h4><span>Case Scenario 3: (poor practice)&nbsp;</span></h4><p><span>A driver is assigned a ‘medium risk rating’ by a fraud detection algorithm after 4 failed trips. Following a further 3 failed trips, he is elevated to a ‘high risk’ and informed via SMS that he is above the local fraud threshold.&nbsp;</span></p>"}]}